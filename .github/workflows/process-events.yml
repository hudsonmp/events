name: Process New Instagram Posts for Events

on:
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of posts to process (default: all unprocessed)'
        required: false
        default: ''
  workflow_run:
    workflows: ["Instagram Scraper (Hourly)"]
    types:
      - completed
  repository_dispatch:
    types: [process-events]

jobs:
  extract-events:
    runs-on: ubuntu-latest
    # Only run if the scraper workflow completed successfully, or if manually triggered
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' || github.event_name == 'repository_dispatch' }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip list  # Debug: show installed packages
    
    - name: Verify environment variables
      run: |
        echo "Checking environment variables..."
        if [ -z "$SUPABASE_PROJECT_URL" ]; then echo "âŒ SUPABASE_PROJECT_URL not set"; exit 1; fi
        if [ -z "$SUPABASE_SERVICE_ROLE" ]; then echo "âŒ SUPABASE_SERVICE_ROLE not set"; exit 1; fi  
        if [ -z "$GROQ_API_KEY" ]; then echo "âŒ GROQ_API_KEY not set"; exit 1; fi
        echo "âœ… All environment variables are set"
        echo "SUPABASE_PROJECT_URL: ${SUPABASE_PROJECT_URL:0:20}..."
      env:
        SUPABASE_PROJECT_URL: ${{ secrets.SUPABASE_PROJECT_URL }}
        SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
    
    - name: Test Supabase connection and check for unprocessed posts
      env:
        SUPABASE_PROJECT_URL: ${{ secrets.SUPABASE_PROJECT_URL }}
        SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
      run: |
        cd instagram/extraction
        python -c "
        import os
        from supabase import create_client
        try:
            client = create_client(os.getenv('SUPABASE_PROJECT_URL'), os.getenv('SUPABASE_SERVICE_ROLE'))
            
            # Test basic connection
            response = client.table('posts').select('id').limit(1).execute()
            print('âœ… Supabase connection successful')
            print(f'Posts table accessible: {len(response.data) >= 0}')
            
            # Check for unprocessed posts
            unprocessed = client.table('posts').select('id').eq('processed', False).limit(5).execute()
            print(f'ðŸ“Š Found {len(unprocessed.data)} unprocessed posts (showing max 5)')
            
            if len(unprocessed.data) == 0:
                print('â„¹ï¸  No unprocessed posts found - extraction will complete quickly')
            else:
                print('ðŸŽ¯ Unprocessed posts found - extraction will proceed')
                
        except Exception as e:
            print(f'âŒ Supabase connection failed: {e}')
            exit(1)
        "

    - name: Run AI Event Extraction
      env:
        SUPABASE_PROJECT_URL: ${{ secrets.SUPABASE_PROJECT_URL }}
        SUPABASE_SERVICE_ROLE: ${{ secrets.SUPABASE_SERVICE_ROLE }}
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      run: |
        cd instagram/extraction
        
        # Create logs directory
        mkdir -p logs
        
        # Determine batch size
        BATCH_SIZE=""
        if [ -n "${{ github.event.inputs.batch_size }}" ]; then
          BATCH_SIZE="${{ github.event.inputs.batch_size }}"
        elif [ -n "${{ github.event.client_payload.batch_size }}" ]; then
          BATCH_SIZE="${{ github.event.client_payload.batch_size }}"
        fi
        
        # Run extraction with logging
        echo "ðŸš€ Starting AI event extraction..."
        if [ -n "$BATCH_SIZE" ]; then
          echo "ðŸ“Š Processing batch size: $BATCH_SIZE"
          python ai.py $BATCH_SIZE 2>&1 | tee logs/extraction-$(date +%Y%m%d-%H%M%S).log
        else
          echo "ðŸ“Š Processing all unprocessed posts"
          python ai.py 2>&1 | tee logs/extraction-$(date +%Y%m%d-%H%M%S).log
        fi
        
        # Check if extraction was successful
        EXTRACTION_EXIT_CODE=${PIPESTATUS[0]}
        if [ $EXTRACTION_EXIT_CODE -eq 0 ]; then
          echo "âœ… Extraction completed successfully"
        else
          echo "âŒ Extraction failed with exit code $EXTRACTION_EXIT_CODE"
          echo "ðŸ“„ Log files created in logs/ directory:"
          ls -la logs/ || echo "No logs directory found"
          echo "ðŸ” Checking for any error patterns in output..."
          # Show last 20 lines of the most recent log file (handles multiple matches safely)
          latest_log=$(ls -t logs/extraction-*.log 2>/dev/null | head -n 1 || true)
          if [ -n "$latest_log" ]; then
            echo "Last 20 lines of $latest_log:"
            tail -n 20 "$latest_log" || echo "Could not read log file"
          fi
          exit $EXTRACTION_EXIT_CODE
        fi
    
    - name: Upload extraction logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: extraction-logs-${{ github.run_number }}
        path: |
          instagram/extraction/logs/*.log
        if-no-files-found: warn
        retention-days: 7 